# FDAT - FastDAT

This is an inspired super resolution arch based on various archs primarily DAT, PLKSR, SPAN and other.
Intended to be a faster version of DAT family of models while being visually and perceptually close.

Work created with the help of LLMs like Claude & Gemini

contact sharekhan. on the Enhance Everything Discord.

# FDAT: Fast Dual Attention Transformer
DAT Inspired Lightweight Attention Network for Real World Image Super Resolution

## Overview

FDAT (FastDAT) is a neural network architecture designed for image super-resolution tasks. It combines the strengths of both attention mechanisms and convolutional operations to effectively upscale low-resolution images while preserving fine details and structural information.

## Architecture Highlights

### ðŸ”„ Dual Attention Mechanism
FDAT employs two complementary attention mechanisms:

- **Spatial Window Attention**: Processes spatial relationships within local windows, reducing computational complexity compared to full spatial attention while maintaining effectiveness
- **Channel Attention**: Focuses on inter-channel relationships to enhance feature representation and selection

### âš¡ Fast Implementation
The "Fast" in FDAT refers to several optimizations:

- **Window-based Spatial Attention**: Instead of computing attention across the entire spatial dimension, it uses local windows (default 8Ã—8), significantly reducing computational cost
- **Simplified Components**: Streamlined implementations of attention and feed-forward modules
- **Efficient Channel Attention**: Uses normalized queries and keys for stable and fast channel-wise attention computation

### ðŸ”— Attention-Convolution Interaction
A key innovation is the **Simplified AIM (Attention Interaction Module)** that intelligently fuses:
- Attention-based global feature understanding
- Convolution-based local feature extraction

This dual pathway allows the model to capture both local textures and global structural patterns effectively.

## How It Works

### Architecture Flow
```
Input Image
    â†“
[Initial Convolution] - Embed input to feature space
    â†“
[Residual Groups] - Multiple groups of dual attention blocks
    â”œâ”€â”€ Spatial Window Attention + Conv â†’ AIM Fusion
    â”œâ”€â”€ Channel Attention + Conv â†’ AIM Fusion  
    â””â”€â”€ FFN with Spatial Mixing
    â†“
[Final Convolution] - Feature refinement
    â†“
[Flexible Upsampler] - Multiple upsampling strategies
    â†“
Output HR Image
```

### Key Components

#### 1. **SimplifiedDATBlock**
The core building block that:
- Alternates between spatial and channel attention based on block pattern
- Runs parallel convolution pathway for local feature extraction
- Fuses attention and convolution features via AIM
- Applies feed-forward network with spatial mixing

#### 2. **FastSpatialWindowAttention**
- Divides input into non-overlapping windows
- Computes self-attention within each window
- Uses learnable relative position bias
- Handles padding for inputs not divisible by window size

#### 3. **FastChannelAttention**
- Normalizes queries and keys for stable training
- Uses temperature-scaled attention for better control
- Focuses on channel-wise feature relationships

#### 4. **SimplifiedFFN**
- Expands feature dimensions for enhanced representation
- Includes spatial mixing via depthwise convolution
- Applies dropout for regularization

#### 5. **Flexible Upsampling**
Supports multiple upsampling strategies:
- **PixelShuffle**: Efficient sub-pixel convolution
- **Transposed Convolution**: Learnable upsampling
- **Nearest + Conv**: Simple interpolation with refinement
- **DySample**: Advanced dynamic sampling (requires spandrel)

## Benefits for Super-Resolution

### ðŸŽ¯ **Local-Global Feature Fusion**
- Convolution captures fine textures and local patterns
- Window attention captures spatial relationships and structures
- Channel attention enhances feature discriminability
- AIM module intelligently combines these complementary representations

### ðŸš€ **Computational Efficiency**
- Window-based attention scales linearly with image size
- Simplified implementations reduce overhead
- Parallel conv-attention pathways enable efficient processing

### ðŸ”§ **Architectural Flexibility**
- Configurable attention patterns (spatial/channel alternation)
- Multiple model sizes (tiny, light, medium, large, xl)
- Various upsampling strategies for different use cases
- Adjustable window sizes and attention heads

### ðŸ“ˆ **Training Stability**
- Stochastic depth (DropPath) for regularization
- Normalized attention for stable gradients
- Residual connections preserve information flow
- Careful weight initialization

## Model Variants

| Model | Embed Dim | Groups | Depth/Group | Heads | Parameters* |
|-------|-----------|--------|-------------|--------|-------------|
| `fdat_tiny` | 96 | 2 | 2 | 3 | ~500K |
| `fdat_light` | 108 | 3 | 2 | 4 | ~800K |
| `fdat_medium` | 120 | 4 | 3 | 4 | ~1.5M |
| `fdat_large` | 180 | 4 | 4 | 6 | ~3.5M |
| `fdat_xl` | 180 | 6 | 6 | 6 | ~7M |

*Approximate parameter counts, varies with configuration

## Usage Example

```python
import torch
from fdat_arch import fdat_tiny, fdat_medium, FDAT

# Use predefined model
model = fdat_tiny(scale=4)

# Or create custom configuration
model = FDAT(
    embed_dim=96,
    num_groups=3,
    depth_per_group=2,
    num_heads=4,
    window_size=8,
    upsampler_type="pixelshuffle",
    scale=4
)

# Super-resolve an image
lr_image = torch.randn(1, 3, 64, 64)  # Low resolution input
with torch.no_grad():
    hr_image = model(lr_image)  # High resolution output (1, 3, 256, 256)
```

## Design Philosophy

FDAT represents a balanced approach to super-resolution that:

1. **Leverages both attention and convolution**: Rather than replacing convolutions entirely, it creates synergy between global attention and local convolution
2. **Prioritizes efficiency**: Uses window attention and optimized implementations to maintain practical computational requirements
3. **Maintains flexibility**: Supports various configurations and upsampling strategies for different use cases
4. **Focuses on stability**: Incorporates modern training techniques like stochastic depth and careful normalization

## Requirements

- PyTorch >= 1.9.0
- numpy
- spandrel (for DySample upsampler)

```bash
pip install torch numpy spandrel
```

## Applications

FDAT is particularly well-suited for:
- **Image Super-Resolution**: Primary use case for enhancing image resolution
- **Image Restoration**: Can be adapted for denoising and artifact removal
- **Real-time Applications**: Efficient architecture suitable for deployment
- **Research**: Flexible foundation for super-resolution research

The architecture's combination of efficiency, effectiveness, and flexibility makes it a strong choice for both practical applications and research in image enhancement tasks.
